<p align="center"><img width="40%" src="logo/pytorch_logo_2018.svg" /></p>

--------------------------------------------------------------------------------

## 梯度下降和线性回归

梯度下降是神经网络中非常重要的计算过程，通过本章希望大家能够学习利用PyTorch实现梯度下降和线性回归。


我们创建一个线性模型，通过查看区域中的平均温度，降雨和湿度（输入变量或）来预测苹果和橙子（目标变量）的作物产量。

线性模型大致如下：
```
yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1
yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2
```
也就是说，苹果的产量是温度，降雨和湿度的线性函数。其中，线性回归的学习部分是使用训练数据来找出一组权重W11，W12，... W23，B1和B2，以对新数据进行准确的预测。首先，我们导入Numpy和PyTorch
```
import numpy as np
import torch
```
### 1. 创建数据
* 输入数据 (5组)
```
# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype='float32')
```
* 预测数据（5组）
```
# Targets (apples, oranges)
targets = np.array([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype='float32')
```

### 2. 将numpy arrays转化为tensor

这是很重要的一步，我们公司的log数据大多需要numpy来进行读取，如果想用PyTorch，就需要将numpy array数据转化为tensor形式的。
```
# 转化为张量（tensors）
inputs = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)
print(inputs)
print(targets)
```
Output:
```
tensor([[ 73.,  67.,  43.],
        [ 91.,  88.,  64.],
        [ 87., 134.,  58.],
        [102.,  43.,  37.],
        [ 69.,  96.,  70.]])
tensor([[ 56.,  70.],
        [ 81., 101.],
        [119., 133.],
        [ 22.,  37.],
        [103., 119.]])
```
线性回归模型当中，weights and biases 也可以表示为矩阵的形式。
```
# Weights and biases
w = torch.randn(2, 3, requires_grad=True)
b = torch.randn(2, requires_grad=True)
print(w)
print(b)
```
Output:
```
tensor([[-0.2910, -0.3450,  0.0305],
        [-0.6528,  0.7386, -0.5153]], requires_grad=True)
tensor([-0.9161, -0.7779], requires_grad=True)
```

### 3. 定义模型
我们定义的线性模型如下：
```
def model(x):
    return x @ w.t() + b
```
其中，@表示pytorch中的矩阵乘法，.t()表示矩阵转置。

我们尝试输入当前模型的预测结果：
```
# Generate predictions
preds = model(inputs)
print(preds)
```
Output:
```
tensor([[-43.9569, -21.1025],
        [-55.7975, -28.1628],
        [-70.6863,  11.5154],
        [-44.2982, -54.6685],
        [-51.9732, -10.9839]], grad_fn=<AddBackward0>)
```
让我们将当前输出和真实的值作对比
```
print(targets)
```
Output:
```
tensor([[ 56.,  70.],
        [ 81., 101.],
        [119., 133.],
        [ 22.,  37.],
        [103., 119.]])
```
我们可以发现当前预测结果和真实值相差甚远

### 4. 损失函数

在改进模型之前，我们需要一种方法来评估模型性能，我们使用均方误差[（MSE）](https://en.wikipedia.org/wiki/Mean_squared_error)来评估模型性能。

定义误差函数如下：

```
# MSE loss
def mse(t1, t2):
    diff = t1 - t2
    return torch.sum(diff * diff) / diff.numel()
```
其中，torch.Sum()返回张量中所有元素的总和。 .numel()返回张量中的元素数量。

接下来，让我们尝试计算loss:
```
loss = mse(preds, targets)
print(loss)
```
Output:
```
tensor(15813.8125, grad_fn=<DivBackward0>)
```

损失函数越小，说明模型效果越好。显然，我们当前训练的模型，不符合我们的预期。

### 5. 梯度下降

我们用[梯度下降](https://www.cnblogs.com/pinard/p/5970503.html)的方法来优化模型，首先，我们要计算损失函数的梯度。
```
loss.backward()
```
梯度存储在相应张量的.grad属性中。
```
print(w)
print(w.grad)
```

Output:
```
tensor([[-0.2910, -0.3450,  0.0305],
        [-0.6528,  0.7386, -0.5153]], requires_grad=True)
tensor([[-10740.7393, -12376.3008,  -7471.2300],
        [ -9458.5078, -10033.8672,  -6344.1094]])
```

我们将梯度乘以很小的数字（一般设置为10^-5），以确保梯度改变的幅度不会太大。 我们想朝梯度的下降的方向迈出一小步，10^-5称为算法的学习率(Learning Rate)。

```
with torch.no_grad():
    w -= w.grad * 1e-5
    b -= b.grad * 1e-5
```
我们在上面更新了参数w和参数b，下面我们看看更新后的参数的表现：
```
loss = mse(preds, targets)
print(loss)
```
Output:
```
tensor(15813.8125, grad_fn=<DivBackward0>)
```

每次参数更新完成后，我们需要调用.zero()将梯度重置为零，以便于下一次新的梯度传入。
```
w.grad.zero_()
b.grad.zero_()
print(w.grad)
print(b.grad)
```
Output:
```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([0., 0.])
```

### 6. 利用梯度下降来优化模型参数
综上所述，我们将使用梯度下降来减少损失并改善模型，步骤如下：



1. 计算损失

2. 计算梯度w.r.t

3. 根据计算的梯度来调整权重

4. 将梯度重置为零


为了能够尽可能的减小损失函数，我们将重复上述过程，下面我们将上述过程迭代100次，每迭代一次称为一个epoch，让我们训练该模型100个epoch。
```
# Train for 100 epochs
for i in range(100):
    preds = model(inputs)
    loss = mse(preds, targets)
    loss.backward()
    with torch.no_grad():
        w -= w.grad * 1e-5
        b -= b.grad * 1e-5
        w.grad.zero_()
        b.grad.zero_()
```

接下来，我们将检测更新过后的权重的训练结果
* Loss:
```
# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
print(loss)
```
Output:
```
tensor(130.3513, grad_fn=<DivBackward0>)
```
我们可以看到，相较于第一次训练的Loss，我们本次训练的Loss显著下降。
* 预测结果：
```
# Predictions
preds
# Targets
targets
```
Output:
```
tensor([[ 60.8975,  70.5663],
        [ 83.9699,  92.9066],
        [108.6802, 150.1993],
        [ 43.5842,  38.4608],
        [ 91.6760, 104.6360]], grad_fn=<AddBackward0>)

tensor([[ 56.,  70.],
        [ 81., 101.],
        [119., 133.],
        [ 22.,  37.],
        [103., 119.]])

```
### 7. PyTorch中的内置功能

我们在上面使用一些基本的张量操作实现了线性回归和梯度下降。 其实Pytorch提供了上述功能，利于我们缩减代码量。
* 导入nn模块
```
import torch.nn as nn
```
##### 创建输入输出，并转化为张量
输入输出一共有15组数据
```
# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70], 
                   [74, 66, 43], 
                   [91, 87, 65], 
                   [88, 134, 59], 
                   [101, 44, 37], 
                   [68, 96, 71], 
                   [73, 66, 44], 
                   [92, 87, 64], 
                   [87, 135, 57], 
                   [103, 43, 36], 
                   [68, 97, 70]], 
                  dtype='float32')

# Targets (apples, oranges)
targets = np.array([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119],
                    [57, 69], 
                    [80, 102], 
                    [118, 132], 
                    [21, 38], 
                    [104, 118], 
                    [57, 69], 
                    [82, 100], 
                    [118, 134], 
                    [20, 38], 
                    [102, 120]], 
                   dtype='float32')

inputs = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)
```

##### Dataset
接下来我们创建一个TensorDataSet，TensordataSet可以将输入和目标作为元组进行访问，并且分其分为训练集和测试集。
```
from torch.utils.data import TensorDataset
```
* 创造训练集
```
# Define dataset
train_ds = TensorDataset(inputs, targets)
train_ds[0:3]
```
Output:
```
(tensor([[ 73.,  67.,  43.],
         [ 91.,  88.,  64.],
         [ 87., 134.,  58.]]),
 tensor([[ 56.,  70.],
         [ 81., 101.],
         [119., 133.]]))
```
##### Dataloader

Dataloader可以在模型训练时将数据分为我们需要的样子，还提供随机抽样数据等多种功能。接下来让我们定义一个dataloader
```
# Define data loader
batch_size = 5
train_dl = DataLoader(train_ds, batch_size, shuffle=True)
```
查看train_dl的内容：
```
for xb, yb in train_dl:
    print(xb)
    print(yb)
    break
```
Output:
```
tensor([[102.,  43.,  37.],
        [ 92.,  87.,  64.],
        [ 87., 134.,  58.],
        [ 69.,  96.,  70.],
        [101.,  44.,  37.]])
tensor([[ 22.,  37.],
        [ 82., 100.],
        [119., 133.],
        [103., 119.],
        [ 21.,  38.]])
```
在每次迭代中，Dataloader返回带有给定批次大小的一批数据。
[Batch size](https://www.google.com/search?q=batchsize&rlz=1C1CHZO_zh-CNHK1017HK1017&oq=batchsize&aqs=chrome.0.69i59j0i10i512j0i512j0i10j0i10i512j0i10l4j0i512.6796j1j7&sourceid=chrome&ie=UTF-8)定义：一次训练所选取的样本数。
Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。

##### nn.Linear
* 我们可以使用Pytorch的NN.Linear类定义模型，这样我们就不用手动初始化权重和偏置了。
```
# Define model
model = nn.Linear(3, 2)
print(model.weight)
print(model.bias)
```
Output:
```
Parameter containing:
tensor([[ 0.1304, -0.1898,  0.2187],
        [ 0.2360,  0.4139, -0.4540]], requires_grad=True)
Parameter containing:
tensor([0.3457, 0.3883], requires_grad=True)
```
* Pytorch 还可以返回模型中的参数：
```
# Parameters
list(model.parameters())
```
Output:
```
[Parameter containing:
 tensor([[ 0.1304, -0.1898,  0.2187],
         [ 0.2360,  0.4139, -0.4540]], requires_grad=True),
 Parameter containing:
 tensor([0.3457, 0.3883], requires_grad=True)]
```
和上面一样，我们可以用该模型来做一些预测
```
# Generate predictions
preds = model(inputs)
preds
```
Output:
```
tensor([[ 6.5493, 25.8226],
        [ 9.5025, 29.2272],
        [-1.0633, 50.0460],
        [13.5738, 25.4576],
        [ 6.4278, 24.6221],
        [ 6.8695, 25.6447],
        [ 9.9110, 28.3593],
        [-0.7142, 49.8280],
        [13.2536, 25.6355],
        [ 6.5161, 23.9321],
        [ 6.9578, 24.9546],
        [ 9.8227, 29.0494],
        [-1.4718, 50.9139],
        [13.4855, 26.1476],
        [ 6.1076, 24.8000]], grad_fn=<AddmmBackward>)
```
##### Loss Function

我们可以直接使用PyTroch中内置的损失函数。
```
# Import nn.functional
import torch.nn.functional as F
```
nn.function中内置了许多损失函数,其中包含了MSE。
```
# Define loss function
loss_fn = F.mse_loss
```
尝试利用上面的损失函数，计算Loss:
```
loss = loss_fn(model(inputs), targets)
print(loss)
```
Output:
```
tensor(5427.9517, grad_fn=<MseLossBackward>)
```
##### Optimizer

我们可以使用优化器 optim.SGD来优化模型，SGD是stochastic gradient descent([随机梯度下降](https://www.jiqizhixin.com/graph/technologies/8e284b12-a865-4915-adda-508a320eefde))的缩写。优化器我们可以理解为优化模型的工具。

```
# Define optimizer
opt = torch.optim.SGD(model.parameters(), lr=1e-5)
```
在上面代码中，model.parameters() 作为参数传递给 optim.SGD，以便优化器知道修改哪些变量，后面的lr=1e-5就是我上面提到的学习率，可以理解为模型参数更新的速度。


### 8. 训练模型
有了上述的前置准备工作我们就可以训练一个模型了。我们的步骤还是和上次训练模型一致：
1. 计算损失

2. 计算梯度w.r.t

3. 根据计算的梯度来调整权重

4. 将梯度重置为零

接下来让我们定义一个fit函数来训练模型

```
def fit(num_epochs, model, loss_fn, opt, train_dl):
    for epoch in range(num_epochs):
        for xb,yb in train_dl:
            pred = model(xb)
            loss = loss_fn(pred, yb)
            loss.backward()
            opt.step()
            opt.zero_grad()
        
        # 打印过程
        if (epoch+1) % 10 == 0:
            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))
```
其中opt.step()的作用是更新模型的参数

我们依旧还是训练100个epoch:
```
fit(100, model, loss_fn, opt, train_dl)
```
Output:
```
Epoch [10/100], Loss: 818.6476
Epoch [20/100], Loss: 335.3347
Epoch [30/100], Loss: 190.3544
Epoch [40/100], Loss: 131.6701
Epoch [50/100], Loss: 77.0783
Epoch [60/100], Loss: 151.5671
Epoch [70/100], Loss: 151.0817
Epoch [80/100], Loss: 67.6262
Epoch [90/100], Loss: 53.6205
Epoch [100/100], Loss: 33.4517
```
将训练好的模型用于预测,并与真实值作比较,发现与真实值相差不多，模型训练成功。
```
preds = model(inputs)
preds
targets
```
Output:
```
tensor([[ 58.4229,  72.0145],
        [ 82.1525,  95.1376],
        [115.8955, 142.6296],
        [ 28.6805,  46.0115],
        [ 97.5243, 104.3522],
        [ 57.3792,  70.9543],
        [ 81.9342,  94.1737],
        [116.2036, 142.6871],
        [ 29.7242,  47.0717],
        [ 98.3498, 104.4486],
        [ 58.2047,  71.0507],
        [ 81.1088,  94.0774],
        [116.1137, 143.5935],
        [ 27.8550,  45.9152],
        [ 98.5680, 105.4124]], grad_fn=<AddmmBackward>)

tensor([[ 56.,  70.],
        [ 81., 101.],
        [119., 133.],
        [ 22.,  37.],
        [103., 119.],
        [ 57.,  69.],
        [ 80., 102.],
        [118., 132.],
        [ 21.,  38.],
        [104., 118.],
        [ 57.,  69.],
        [ 82., 100.],
        [118., 134.],
        [ 20.,  38.],
        [102., 120.]])
```


